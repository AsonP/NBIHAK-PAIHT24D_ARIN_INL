import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import gradio as gr

# Ladda milj√∂variabler fr√•n .env
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Konfiguration
CACHE_FILE = "data/jevees_cache.json"
BACKGROUND_FILE = "data/jevees_background.json"
INDEX_FILE = "data/index.faiss"
EMBEDDING_DIM = 768

# Initiera SentenceTransformer f√∂r embeddings
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

# Ladda JSON-data
background_data = json.load(open(BACKGROUND_FILE, encoding="utf-8"))
traits = background_data.get("traits", {})

def format_traits(traits):
    return "\n".join([f"- {key.capitalize()}: {value}" for key, value in traits.items()])

def format_chat_history(history):
    if not history:
        return "Ingen tidigare konversation."
    formatted = ""
    for msg in history:
        if isinstance(msg, HumanMessage):
            formatted += f"Anv√§ndaren: {msg.content}\n"
        elif isinstance(msg, AIMessage):
            formatted += f"Jevees: {msg.content}\n"
    return formatted.strip()

def validate_json_file(filepath, default_data=None):
    if not os.path.exists(filepath):
        print(f"{filepath} saknas ‚Äì skapar en ny...")
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(default_data if default_data else {}, f, ensure_ascii=False, indent=4)
        return default_data if default_data else {}
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            if not isinstance(data, dict) and not isinstance(data, list):
                print(f"{filepath} var inte ett giltigt JSON-objekt ‚Äì √•terst√§lls.")
                return default_data if default_data else {}
            return data
    except json.JSONDecodeError:
        print(f"{filepath} var korrupt ‚Äì skapar en ny...")
        return default_data if default_data else {}

def validate_faiss_index(index_path, embedding_dim):
    if not os.path.exists(index_path):
        print(f"{index_path} saknas ‚Äì skapar en ny FAISS-index...")
        index = faiss.IndexFlatL2(embedding_dim)
        faiss.write_index(index, index_path)
        return index
    try:
        index = faiss.read_index(index_path)
        print(f"FAISS-indexet {index_path} laddat framg√•ngsrikt.")
        print(f"Embedding-dimension i index: {index.d}")
        return index
    except Exception as e:
        print(f"Fel vid l√§sning av {index_path}: {e} ‚Äì skapar ett nytt FAISS-index...")
        index = faiss.IndexFlatL2(embedding_dim)
        faiss.write_index(index, index_path)
        return index

def retrieve_relevant_text(query, index, texts, top_k=2):
    query_embedding = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, top_k)
    if len(indices) == 0 or len(indices[0]) == 0:
        return ["Jag har tyv√§rr ingen specifik information om det, men jag kan ge en generell rekommendation."]
    return [texts[i] for i in indices[0] if i < len(texts)]

# Validera filer
cache = validate_json_file(CACHE_FILE, {})
background_data = validate_json_file(BACKGROUND_FILE, {"background": [], "traits": {}})
index = validate_faiss_index(INDEX_FILE, EMBEDDING_DIM)

# Skapa embeddings och FAISS-index
texts = [entry["content"] for entry in background_data["background"]]
embeddings = model.encode(texts, convert_to_numpy=True)
print(f"Embedding-dimension fr√•n modell: {embeddings.shape[1]}")
if index.d != embeddings.shape[1]:
    print(f"Dimension mismatch! Index: {index.d}, Embeddings: {embeddings.shape[1]} ‚Äì Skapar nytt index.")
    index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)
faiss.write_index(index, INDEX_FILE)

# Skapa minne med InMemoryChatMessageHistory
chat_history = InMemoryChatMessageHistory()

# Dynamisk prompt
prompt_template = PromptTemplate(
    input_variables=["chat_history", "context", "question", "traits"],
    template="""
    üìå Du √§r Jevees, en sofistikerad butler med f√∂ljande egenskaper:
    {traits}
    
    üìú Tidigare konversation:
    {chat_history}
    
    üìö Historik:
    {context}
    
    Om anv√§ndaren st√§ller en direkt f√∂ljdfr√•ga, h√•ll svaret kortfattat och undvik upprepning.
    Svara p√• samma spr√•k som fr√•gan √§r st√§lld p√• (t.ex. svenska f√∂r svenska fr√•gor, engelska f√∂r engelska fr√•gor).
    
    Baserat p√• detta, svara p√• fr√•gan:
    "{question}"
    """
)

# Skapa LLM och bas-kedja
llm = ChatOpenAI(model_name="gpt-4o", openai_api_key=OPENAI_API_KEY)
base_chain = prompt_template | llm

# Skapa en kedja med minneshantering
chain_with_history = RunnableWithMessageHistory(
    base_chain,
    lambda session_id: chat_history,
    input_messages_key="question",
    history_messages_key="chat_history"
)

def chat_with_jevees(user_input, history):
    if not user_input:
        return history
    
    # Om anv√§ndaren skriver "exit", "quit" eller "avsluta", avsluta konversationen
    if user_input.lower() in ["exit", "quit", "avsluta"]:
        history.append(("Du", user_input))
        history.append(("Jevees", "Ha en fortsatt trevlig dag!"))
        return history
    
    # H√§mta relevant kontext
    relevant_info = retrieve_relevant_text(user_input, index, texts)
    context = "\n".join(relevant_info)
    
    # Formatera chat-historiken (begr√§nsa till 5 meddelanden)
    chat_history_formatted = format_chat_history(chat_history.messages[:5])
    
    # K√∂r kedjan med historik
    response = chain_with_history.invoke(
        {
            "chat_history": chat_history_formatted,
            "context": context,
            "question": user_input,
            "traits": format_traits(traits)
        },
        config={"configurable": {"session_id": "default"}}
    )
    
    # Spara i minnet
    chat_history.add_user_message(user_input)
    chat_history.add_ai_message(response.content)
    
    # Spara i cache
    cache[user_input] = response.content
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=4)
    
    # Uppdatera Gradio-historiken
    history.append(("Du", user_input))
    history.append(("Jevees", response.content))
    return history

# Skapa Gradio-gr√§nssnitt med anpassad bakgrundsf√§rg
with gr.Blocks(
    title="Jevees ‚Äì Din Sofistikerade Butler",
    theme=gr.themes.Default(primary_hue="blue").set(
        body_background_fill="#f5f5f5"  # Ljusgr√• bakgrund f√∂r b√§ttre l√§sbarhet
    )
) as demo:
    gr.Markdown("# üìå V√§lkommen till Jevees!", container=False)
    gr.Markdown("St√§ll en fr√•ga till Jevees, din personliga butler. Han svarar p√• samma spr√•k som din fr√•ga!", container=False)
    
    chatbot = gr.Chatbot(label="üí¨ Konversation")
    msg = gr.Textbox(label="‚úçÔ∏è Skriv din fr√•ga h√§r", placeholder="Vad vill du veta?")
    clear = gr.Button("üóëÔ∏è Rensa konversationen")
    
    # Koppla funktionen till textboxen
    msg.submit(chat_with_jevees, inputs=[msg, chatbot], outputs=[chatbot])
    
    # Rensa historiken
    clear.click(lambda: [], outputs=[chatbot])

# Starta gr√§nssnittet
demo.launch()